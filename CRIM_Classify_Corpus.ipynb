{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Classify a Group of Pieces to ONE output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "music21: Certain music21 functions might need the optional package matplotlib;\n",
      "                  if you run into errors, install it by following the instructions at\n",
      "                  http://mit.edu/music21/doc/installing/installAdditional.html\n"
     ]
    }
   ],
   "source": [
    "from intervals.main_objs import *\n",
    "from intervals.main import *\n",
    "# import re\n",
    "# from crim_intervals import *\n",
    "import pandas as pd\n",
    "import ast\n",
    "# import matplotlib\n",
    "from itertools import tee, combinations\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "import re\n",
    "# from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Basic Parameters\n",
    "\n",
    "min_exact_matches = 2\n",
    "min_close_matches = 3\n",
    "close_distance = 1\n",
    "vector_size = 4\n",
    "increment_size = 4\n",
    "forward_gap_limit = 40\n",
    "backward_gap_limit = 40\n",
    "min_sum_durations = 10\n",
    "max_sum_durations = 30\n",
    "offset_difference_limit = 500\n",
    "\n",
    "duration_type = \"real\"\n",
    "interval_type = \"generic\"\n",
    "match_type = \"close\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "crim = 'https://raw.githubusercontent.com/CRIM-Project/CRIM-online/master/crim/static/mei/MEI_3.0/'\n",
    "\n",
    "git = 'https://raw.githubusercontent.com/RichardFreedman/CRIM_additional_works/main/'\n",
    "\n",
    "def batch_classify(corpus_titles, duration_type=\"real\", interval_type=\"generic\", match_type=\"close\"):\n",
    "\n",
    "    for title in titles:\n",
    "        path = f\"{crim}{title}\"\n",
    "        clean_title = re.search(\"[a-zA-Z_\\d]+\", title).group()\n",
    "        \n",
    "        corpus = CorpusBase([path])\n",
    "#         corpus = CorpusBase(corpus_titles)\n",
    "\n",
    "        if duration_type == \"real\":\n",
    "\n",
    "            vectors = IntervalBase(corpus.note_list)\n",
    "\n",
    "        elif duration_type == \"incremental\":\n",
    "\n",
    "            vectors = IntervalBase(corpus.note_list_incremental_offset(increment_size))\n",
    "\n",
    "        if interval_type == \"generic\":\n",
    "\n",
    "            patterns = into_patterns([vectors.generic_intervals], vector_size)\n",
    "\n",
    "        elif interval_type == \"semitone\":\n",
    "\n",
    "            patterns = into_patterns([vectors.semitone_intervals], vector_size)\n",
    "\n",
    "        if match_type == \"exact\":\n",
    "\n",
    "            exact_matches = find_exact_matches(patterns, min_exact_matches)\n",
    "            output_exact = export_pandas(exact_matches)\n",
    "            df = output_exact\n",
    "            pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "            df[\"note_durations\"] = df[\"note_durations\"].map(lambda x: pd.eval(x))\n",
    "            df[\"start_offset\"] = df[\"start_offset\"].map(lambda x: pd.eval(x))\n",
    "            df[\"end_offset\"] = df[\"end_offset\"].map(lambda x: pd.eval(x))\n",
    "            df[\"pattern_generating_match\"] = df[\"pattern_generating_match\"].apply(tuple)\n",
    "            df[\"pattern_matched\"] = df[\"pattern_matched\"].apply(tuple)\n",
    "            df[\"sum_durs\"] = df.note_durations.apply(sum)\n",
    "            df = df.round(2)\n",
    "\n",
    "        elif match_type == \"close\":\n",
    "\n",
    "            close_matches = find_close_matches(patterns, min_close_matches, close_distance)\n",
    "            output_close = export_pandas(close_matches)\n",
    "            output_close[\"pattern_generating_match\"] = output_close[\"pattern_generating_match\"].apply(tuple)\n",
    "            df = output_close\n",
    "            pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "            df[\"note_durations\"] = df[\"note_durations\"].map(lambda x: pd.eval(x))\n",
    "            df[\"start_offset\"] = df[\"start_offset\"].map(lambda x: pd.eval(x))\n",
    "            df[\"end_offset\"] = df[\"end_offset\"].map(lambda x: pd.eval(x))\n",
    "            df[\"pattern_generating_match\"] = df[\"pattern_generating_match\"].apply(tuple)\n",
    "            df[\"pattern_matched\"] = df[\"pattern_matched\"].apply(tuple)\n",
    "            df[\"sum_durs\"] = df.note_durations.apply(sum)\n",
    "            df = df.round(2)\n",
    "\n",
    "        df2 = df\n",
    "\n",
    "        # Make Groups, Sort By Group and Offset, then and Add Previous/Next\n",
    "        df2[\"group_number\"] = df2.groupby('pattern_matched').ngroup()\n",
    "        df2 = df2.sort_values(['group_number', 'start_offset'])\n",
    "        df2[\"prev_entry_off\"] = df2[\"start_offset\"].shift(1)\n",
    "        df2[\"next_entry_off\"] = df2[\"start_offset\"].shift(-1)\n",
    "\n",
    "\n",
    "        first_of_group = df2.drop_duplicates(subset=[\"pattern_matched\"], keep='first').index\n",
    "        df2[\"is_first\"] = df2.index.isin(first_of_group)\n",
    "        last_of_group = df2.drop_duplicates(subset=[\"pattern_matched\"], keep='last').index\n",
    "        df2[\"is_last\"] = df2.index.isin(last_of_group)\n",
    "\n",
    "        # Check Differences between Next and Last Offset\n",
    "\n",
    "        df2[\"last_off_diff\"] = df2[\"start_offset\"] - df2[\"prev_entry_off\"]\n",
    "        df2[\"next_off_diff\"] = df2[\"next_entry_off\"] - df2[\"start_offset\"]\n",
    "\n",
    "        # Find Parallel Entries \n",
    "        df2[\"parallel\"] = df2[\"last_off_diff\"] == 0\n",
    "\n",
    "        # Set Gap Limits and Check Gaps Forward and Back\n",
    "        df2[\"forward_gapped\"] = df2[\"next_off_diff\"] >= forward_gap_limit\n",
    "        df2[\"back_gapped\"] = df2[\"last_off_diff\"] >= backward_gap_limit\n",
    "\n",
    "        # Find Singletons and Split Groups with Gaps\n",
    "        df2[\"singleton\"] = ((df2['forward_gapped'] == True) & (df2['back_gapped'] == True) | (df2['back_gapped'] == True) & (df2[\"is_last\"]))\n",
    "        df2[\"split_group\"] = (df2['forward_gapped'] == False) & (df2['back_gapped'] == True)\n",
    "\n",
    "        #Mask Out Parallels and Singletons\n",
    "        df2 = df2[df2[\"parallel\"] != True]\n",
    "        df2 = df2[df2[\"singleton\"] != True]\n",
    "        df2[\"next_off_diff\"] = df2[\"next_off_diff\"].abs()\n",
    "        df2[\"last_off_diff\"] = df2[\"last_off_diff\"].abs()\n",
    "\n",
    "        # Find Final Groups\n",
    "        df2[\"combined_group\"] = (df2.split_group | df2.is_first)\n",
    "        df2.loc[(df2[\"combined_group\"]), \"sub_group_id\"] = range(df2.combined_group.sum())\n",
    "        df2[\"sub_group_id\"] = df2[\"sub_group_id\"].ffill()\n",
    "\n",
    "        ###\n",
    "        ### FILTER SHORT OR LONG ENTRIES\n",
    "        ###\n",
    "        df2 = df2[df2[\"sum_durs\"] >= min_sum_durations]\n",
    "        df2 = df2[df2[\"sum_durs\"] <= max_sum_durations]\n",
    "\n",
    "        classified2 = df2.applymap(lists_to_tuples).groupby(\"sub_group_id\").apply(predict_type)\n",
    "\n",
    "        # OPTIONAL:  drop the new singletons\n",
    "\n",
    "        classified2.drop(classified2[classified2['predicted_type'] == \"Singleton\"].index, inplace = True)\n",
    "\n",
    "        # OPTIONAL:  select only certain presentation types\n",
    "\n",
    "        # classified2 = classified2[classified2[\"predicted_type\"] == \"PEN\"]\n",
    "\n",
    "        classified2[\"start\"] = classified2[\"start_measure\"].astype(str) +\"/\"+ classified2[\"start_beat\"].astype(str) \n",
    "        classified2.drop(columns=['start_measure', 'start_beat','offset_diffs'], inplace=True)\n",
    "\n",
    "\n",
    "        # put things back in order by offset and group them again\n",
    "        classified2.sort_values(by = [\"start_offset\"], inplace=True)\n",
    "\n",
    "        # Now transform as Pivot Table\n",
    "        pivot = classified2.pivot_table(index=[\"piece_title\", \"pattern_generating_match\", \"pattern_matched\", \"predicted_type\", \"sub_group_id\"],\n",
    "                    columns=\"entry_number\",\n",
    "                    values=[\"part\", \"start_offset\", \"start\", \"sum_durs\"],\n",
    "                    aggfunc=lambda x: x)\n",
    "        pivot_sort = pivot.sort_values(by = [(\"start_offset\", 1)])\n",
    "        pivot_sort = pivot_sort.fillna(\"-\")\n",
    "        pivot_sort.reset_index(inplace=True)\n",
    "        pivot_sort = pivot_sort.drop(columns=['start_offset', \"sub_group_id\"], level=0)\n",
    "\n",
    "        # group by patterns and minimum of two pieces\n",
    "\n",
    "    #     pivot_sort[\"pattern_matched\"] = pivot_sort.pattern_matched.apply(pd.eval).apply(tuple)\n",
    "\n",
    "    #     pivot_sort[\"unique_titles_for_pattern\"] = pivot_sort.groupby(\"pattern_matched\").piece_title.transform(lambda group: group.nunique())\n",
    "\n",
    "    #     p2 = pivot_sort[pivot_sort.unique_titles_for_pattern > 1]\n",
    "\n",
    "    #     p3 = p2.sort_values(\"pattern_matched\")\n",
    "    #     \n",
    "    #     p3.to_csv(\"corpus_classified.csv\")\n",
    "    #   \n",
    "        \n",
    "#         pivot_sort.to_csv(f\"{clean_title}_{interval_type}_{match_type}_{duration_type}.csv\")\n",
    "        return pivot_sort\n",
    "   \n",
    "\n",
    "# Converts lists to tuples\n",
    "\n",
    "def lists_to_tuples(el):\n",
    "    if isinstance(el, list):\n",
    "        return tuple(el)\n",
    "    else:\n",
    "        return el\n",
    "\n",
    "# Filters for the length of the Presentation Type in the Classifier\n",
    "\n",
    "def limit_offset_size(array, limit):\n",
    "    under_limit = np.cumsum(array) <= limit\n",
    "    return array[: sum(under_limit)]\n",
    "\n",
    "# Gets the the list of offset differences for each group \n",
    "\n",
    "def get_offset_difference_list(group):\n",
    "    # if we do sort values as part of the func call, then we don't need this first line\n",
    "    group = group.sort_values(\"start_offset\")\n",
    "    group[\"next_offset\"] = group.start_offset.shift(-1)\n",
    "    offset_difference_list = (group.next_offset - group.start_offset).dropna().tolist()\n",
    "    return offset_difference_list\n",
    "\n",
    "# The classifications are done here\n",
    "# be sure to have the offset difference limit set here and matched in gap check below  80 = ten bars\n",
    "\n",
    "def classify_offsets(offset_difference_list):\n",
    "    \"\"\"\n",
    "    Put logic for classifying an offset list here\n",
    "    \"\"\"\n",
    "    # \n",
    "    offset_difference_list = limit_offset_size(offset_difference_list, offset_difference_limit)\n",
    "    \n",
    "    alt_list = offset_difference_list[::2]\n",
    "    \n",
    "    if len(set(offset_difference_list)) == 1 and len(offset_difference_list) > 1:\n",
    "        return (\"PEN\", offset_difference_list)\n",
    "    # elif (len(offset_difference_list) %2 != 0) and (len(set(alt_list)) == 1):\n",
    "    elif (len(offset_difference_list) %2 != 0) and (len(set(alt_list)) == 1) and (len(offset_difference_list) >= 3):\n",
    "        return (\"ID\", offset_difference_list)\n",
    "    elif len(offset_difference_list) >= 1:\n",
    "        return (\"Fuga\", offset_difference_list)\n",
    "    else: \n",
    "        return (\"Singleton\", offset_difference_list)\n",
    "    \n",
    "# adds predicted type, offsets and entry numbers to the results\n",
    "\n",
    "def predict_type(group):\n",
    "    offset_differences = get_offset_difference_list(group)\n",
    "    predicted_type, offsets = classify_offsets(offset_differences)\n",
    "\n",
    "    group[\"predicted_type\"] = [predicted_type for i in range(len(group))]\n",
    "    group[\"offset_diffs\"] = [offsets for i in range(len(group))]\n",
    "    group[\"entry_number\"] = [i + 1 for i in range(len(group))]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoized piece detected...\n",
      "Finding close matches...\n",
      "187 melodic intervals had more than 3 exact or close matches.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# titles = ['CRIM_Mass_0015_2.mei']\n",
    "\n",
    "\n",
    "titles = ['CRIM_Model_0017.mei']\n",
    "          \n",
    "# 'CRIM_Mass_0021_1.mei', \n",
    "# 'CRIM_Mass_0021_2.mei', 'CRIM_Mass_0021_3.mei', 'CRIM_Mass_0021_4.mei', 'CRIM_Mass_0021_5.mei']\n",
    "\n",
    "# titles = ['Riquet_Missa_Susanne_1.mei_msg.mei', \n",
    "# 'Riquet_Missa_Susanne_2.mei_msg.mei', \n",
    "# 'Riquet_Missa_Susanne_3.mei_msg.mei', \n",
    "# 'Riquet_Missa_Susanne_4.mei_msg.mei', \n",
    "# 'Riquet_Missa_Susanne_5.mei_msg.mei']\n",
    "\n",
    "\n",
    "batch_classify(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CRIM_Intervals",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
